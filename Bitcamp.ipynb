{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Flatten, Dense, Lambda, Convolution2D, Cropping2D, Dropout, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from keras import backend as K\n",
    "import glob\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_test_data(test_size=0.2):\n",
    "    car_files = glob.glob(\"resizedImage\\cars\\*\")\n",
    "    artilary_files = glob.glob(\"resizedImage\\militaryArtillary\\*\")\n",
    "    tank_files = glob.glob(\"resizedImage\\militaryTank\\*\")\n",
    "    truck_files = glob.glob(\"resizedImage\\militaryTruck\\*\")\n",
    "\n",
    "    # Restrict cat and dog files here for testing\n",
    "    #cat_list = [convert_image_to_data(i, num_pix, num_pix) for i in cat_files]\n",
    "    #dog_list = [convert_image_to_data(i, num_pix, num_pix) for i in dog_files]\n",
    "\n",
    "    y_car = np.zeros(len(car_files))\n",
    "    y_artilary = np.ones(len(artilary_files))\n",
    "    y_tank = np.ones(len(tank_files))\n",
    "    y_truck = np.ones(len(truck_files))\n",
    "\n",
    "    \"\"\"\n",
    "    print(len(car_files))\n",
    "    print(len(artilary_files))\n",
    "    print(len(tank_files))\n",
    "    print(len(truck_files))\n",
    "    \"\"\"\n",
    "    X = np.concatenate([car_files, artilary_files, tank_files, truck_files])\n",
    "    \"\"\"\n",
    "    print(len(X))\n",
    "    print(X[0])\n",
    "    \"\"\"\n",
    "    print(\"Total Dataset = \", X.shape)\n",
    "    y = np.concatenate([y_car, y_artilary, y_tank, y_truck])\n",
    "    print(\"Total Labels = \", len(y))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)                                                \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset =  (3186,)\n",
      "Total Labels =  3186\n",
      "(2548,)\n",
      "(2548,)\n",
      "(638,)\n",
      "(638,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = create_train_test_data()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2f4273e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hist = np.histogram(y_train, bins=2,range = (0,1))\n",
    "plt.hist(y_train, bins='auto')\n",
    "plt.hist(y_test, bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a generator\n",
    "def generator(X,y, batch_size=32):\n",
    "    num_samples = len(X)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        combined = list(zip(X,y))\n",
    "        sklearn.utils.shuffle(combined) # shuffle data\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = combined[offset:offset+batch_size] # batch of 32 feeded in the batch samples\n",
    "            #batch_Y = Y[offset:offset+batch_size]\n",
    "            images = [] # empty arrays for processing\n",
    "            labels = []\n",
    "            for batch_sample in batch_samples:\n",
    "                #print( batch_sample[0].split('/')[-1])\n",
    "                #name = '../data/IMG/'+batch_sample[0].split('\\\\')[-1] #batch_sample[0].split('/')[-1] # <--- should be used if code running on windows instead of EC2\n",
    "                center_image = cv2.imread(batch_sample[0])\n",
    "                center_image = cv2.cvtColor(center_image, cv2.COLOR_BGR2RGB) # takes care of the BGR to RGB conversion. since finally the simulator outputs RGB\n",
    "                #center_angle = float(batch_sample[1])\n",
    "                images.append(center_image)\n",
    "                labels.append(batch_sample[1])\n",
    "\n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(labels)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train) # shuffle data again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile and train the model using the generator function\n",
    "train_generator = generator(X_train, y_train, batch_size=32)\n",
    "#validation_generator = generator(validation_samples, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kapil\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (5, 5), strides=(2, 2), activation=\"relu\")`\n",
      "  import sys\n",
      "C:\\Users\\kapil\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(36, (5, 5), strides=(2, 2), activation=\"relu\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\kapil\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (5, 5), strides=(2, 2), activation=\"relu\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\kapil\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\kapil\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\kapil\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.5)`\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Preprocess incoming data, centered around zero with small standard deviation \n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(256,256,3)))\n",
    "# Cropping to focus only on road\n",
    "#model.add(Cropping2D(cropping=((50,20), (0,0))))\n",
    "#Covolution Layer 1 \n",
    "model.add(Convolution2D(24, 5, 5, activation='relu', subsample=(2, 2)))\n",
    "#Covolution Layer 2\n",
    "model.add(Convolution2D(36, 5, 5, activation='relu', subsample=(2, 2)))\n",
    "#Covolution Layer 3\n",
    "model.add(Convolution2D(48, 5, 5, activation='relu', subsample=(2, 2)))\n",
    "#Covolution Layer 4\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "#Covolution Layer 5\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "#Dropout Layer \n",
    "model.add(Dropout(p = 0.5)) # can be tuned\n",
    "# Flatten for passing to Fully Connected layers\n",
    "model.add(Flatten())\n",
    "# FC 1\n",
    "model.add(Dense(100))#, activation='elu')) <--- Can add relu here too. But I thought it wasn't necessary later on.\n",
    "# FC 2\n",
    "model.add(Dense(50))#, activation='elu'))\n",
    "# FC 3\n",
    "model.add(Dense(10))#, activation='elu'))\n",
    "# FC 4\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kapil\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "C:\\Users\\kapil\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=2548, epochs=13)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "2548/2548 [==============================] - 153s 60ms/step - loss: 0.0095 - acc: 0.9999\n",
      "Epoch 2/13\n",
      "2548/2548 [==============================] - 151s 59ms/step - loss: 0.0089 - acc: 0.9999\n",
      "Epoch 3/13\n",
      "2548/2548 [==============================] - 152s 60ms/step - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 4/13\n",
      "2548/2548 [==============================] - 162s 63ms/step - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 5/13\n",
      "2548/2548 [==============================] - 152s 60ms/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 6/13\n",
      "2548/2548 [==============================] - 152s 59ms/step - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 7/13\n",
      "2548/2548 [==============================] - 152s 60ms/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 8/13\n",
      "2548/2548 [==============================] - 152s 60ms/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 9/13\n",
      "2548/2548 [==============================] - 153s 60ms/step - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 10/13\n",
      "2548/2548 [==============================] - 151s 59ms/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 11/13\n",
      "2548/2548 [==============================] - 151s 59ms/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 12/13\n",
      "2548/2548 [==============================] - 153s 60ms/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 13/13\n",
      "2548/2548 [==============================] - 155s 61ms/step - loss: 0.0024 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dca81eeda0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy']) # Using default learning rate and MSE as loss function\n",
    "model.fit_generator(train_generator, samples_per_epoch= len(X_train), nb_epoch=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a generator\n",
    "def testCreator(X):\n",
    "    images = []\n",
    "    num_samples = len(X)\n",
    "    for path in X:\n",
    "        #print( batch_sample[0].split('/')[-1])\n",
    "        #name = '../data/IMG/'+batch_sample[0].split('\\\\')[-1] #batch_sample[0].split('/')[-1] # <--- should be used if code running on windows instead of EC2\n",
    "        center_image = cv2.imread(path)\n",
    "        center_image = cv2.cvtColor(center_image, cv2.COLOR_BGR2RGB) # takes care of the BGR to RGB conversion. since finally the simulator outputs RGB\n",
    "        #center_angle = float(batch_sample[1])\n",
    "        images.append(center_image)\n",
    "        # trim image to only see section with road\n",
    "    X_test = np.array(images)\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638/638 [==============================] - 1s 1ms/step\n",
      "(638, 1)\n",
      "[0.95611285]\n"
     ]
    }
   ],
   "source": [
    "#model.evaluate(X_test, y = y_test,batch_size=638, verbose=1)\n",
    "PREDICTED_CLASSES = model1.predict_classes(testCreator(X_test), batch_size=638, verbose=1)\n",
    "#print(type(PREDICTED_CLASSES))\n",
    "#PREDICTED_CLASSES = (PREDICTED_CLASSES>=0.5)\n",
    "y_test = y_test.reshape(638,1)\n",
    "temp = sum(y_test == PREDICTED_CLASSES)/len(y_test)\n",
    "print(y_test.shape)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_frames_to_video():\n",
    "    output = glob.glob(\"output_M\\*\")\n",
    "    frame_array = []\n",
    "    #print(frame_array[0])\n",
    "    \n",
    "    #out = cv2.VideoWriter(pathOut,cv2.VideoWriter_fourcc(*'DIVX'), fps, (600,600))\n",
    "    for i in output:\n",
    "        # writing to a image array\n",
    "        img = cv2.imread(i)\n",
    "        frame_array.append(img)\n",
    "        #cv2.resizeWindow('image', 1000,1000)\n",
    "        #cv2.imshow('image',frame_array[i])    \n",
    "        #cv2.waitKey(500)\n",
    "        #cv2.destroyAllWindows()\n",
    "    out = cv2.VideoWriter('D:\\output.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (600,600))\n",
    "    for i in range(len(frame_array)):\n",
    "        out.write(frame_array[i])\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638/638 [==============================] - 1s 896us/step\n",
      "(638, 1)\n",
      "[0.95611285]\n"
     ]
    }
   ],
   "source": [
    "PREDICTED_CLASSES = model1.predict_classes(testCreator(X_test), batch_size=638, verbose=1)\n",
    "#print(type(PREDICTED_CLASSES))\n",
    "#PREDICTED_CLASSES = (PREDICTED_CLASSES>=0.5)\n",
    "y_test = y_test.reshape(638,1)\n",
    "temp = sum(y_test == PREDICTED_CLASSES)/len(y_test)\n",
    "print(y_test.shape)\n",
    "print(temp)\n",
    "\n",
    "\n",
    "\n",
    "images = testCreator(X_test)\n",
    "\n",
    "font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "bottomLeftCornerOfText = (10,150)\n",
    "fontScale              = 1\n",
    "fontColor              = (255,255,255)\n",
    "lineType               = 2\n",
    "\n",
    "combined = zip(images,PREDICTED_CLASSES)\n",
    "frame_array = []\n",
    "it = 0\n",
    "for i in combined:\n",
    "    \n",
    "    cv2.namedWindow('image',cv2.WINDOW_NORMAL)\n",
    "    img = cv2.cvtColor(i[0], cv2.COLOR_BGR2RGB)\n",
    "    if(i[1] == 1):\n",
    "                    cv2.putText(img,'Military Vechile', \n",
    "                    bottomLeftCornerOfText, \n",
    "                    font, \n",
    "                    fontScale,\n",
    "                    fontColor,\n",
    "                    lineType)\n",
    "    else:\n",
    "                  cv2.putText(img,'Not a Military Vechile', \n",
    "                    bottomLeftCornerOfText, \n",
    "                    font, \n",
    "                    fontScale,\n",
    "                    fontColor,\n",
    "                    lineType)\n",
    "    cv2.imwrite(\"output_M/\" + str(it)+\".png\",img)\n",
    "    it = it+1\n",
    "    frame_array.append(img)\n",
    "    #cv2.resizeWindow('image', 1000,1000)\n",
    "    #cv2.imshow('image',img)    \n",
    "    #cv2.waitKey(33)\n",
    "    #cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
